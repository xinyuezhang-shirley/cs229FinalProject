{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen-assisted Poem â†’ Song Matching (indexes aligned)\n",
        "\n",
        "This notebook loads the existing poem/song data, shortlists candidates with MPNet cosine, then asks a local Qwen model to score emotional/thematic fit. It saves JSONL with `poem_index`, `song_index_aligned`, `label`, titles/artists, and recursively replaces any flagged bad pairs.\n",
        "\n",
        "**Setup requirements**\n",
        "- `transformers`, `torch` installed.\n",
        "- A local Qwen model (e.g., `Qwen/Qwen2.5-7B-Instruct`) available at `QWEN_MODEL_PATH` or `QWEN_MODEL_ID`. No external calls are made; you must have the weights locally.\n",
        "- Existing embeddings/files in this repo: `data/raw/poetrydb_poems.json`, `data/processed/combined_songs_large_fixed.json`, `data/processed/additional_features.npz`, `data/processed/mpnet_embeddings_{poems,songs}.npy`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "51cdb31f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Poems: 3413 | Songs aligned: 2934\n"
          ]
        }
      ],
      "source": [
        "import json, os, re, math, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "\n",
        "poems = json.load((DATA_DIR / \"raw\" / \"poetrydb_poems.json\").open())['items']\n",
        "poem_texts = [\" \".join(p.get(\"lines\", [])) for p in poems]\n",
        "poem_titles = [p.get(\"title\", \"\") for p in poems]\n",
        "\n",
        "raw_songs = json.load((DATA_DIR / \"processed\" / \"combined_songs_large_fixed.json\").open())['items']\n",
        "song_titles = [s.get(\"title\", \"\") for s in raw_songs]\n",
        "song_artists = [s.get(\"artist\", \"\") for s in raw_songs]\n",
        "song_lyrics = [s.get(\"lyrics\", \"\") or \"\" for s in raw_songs]\n",
        "\n",
        "feats = np.load(DATA_DIR / \"processed\" / \"additional_features.npz\", allow_pickle=True)\n",
        "songs_source_indexes = feats[\"songs_source_indexes\"]\n",
        "\n",
        "# Load embeddings and align songs to filtered order\n",
        "poem_vecs = np.load(DATA_DIR / \"processed\" / \"mpnet_embeddings_poems.npy\")\n",
        "song_vecs_raw = np.load(DATA_DIR / \"processed\" / \"mpnet_embeddings_songs.npy\")\n",
        "song_vecs = song_vecs_raw[songs_source_indexes]\n",
        "\n",
        "# Normalize embeddings for cosine\n",
        "poem_norm = poem_vecs / np.linalg.norm(poem_vecs, axis=1, keepdims=True)\n",
        "song_norm = song_vecs / np.linalg.norm(song_vecs, axis=1, keepdims=True)\n",
        "\n",
        "print(f\"Poems: {len(poems)} | Songs aligned: {len(song_vecs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d730b9e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emotion lexicon and helper for a quick affect vector (used in prompts and backup scoring)\n",
        "emotion_bins = {\n",
        "    'joy': ['joy','happy','bright','delight','hope','sun','light','laugh','smile'],\n",
        "    'sadness': ['sad','sorrow','tears','weep','cry','grief','lonely','loss','blue','pain','ache'],\n",
        "    'love': ['love','heart','dear','beloved','kiss','tender','desire','romance','affection','darling'],\n",
        "    'anger': ['rage','anger','wrath','fury','hate','storm','fight','shout'],\n",
        "    'fear': ['fear','dread','dark','haunt','ghost','terror','nightmare','afraid','alone'],\n",
        "    'wonder': ['wonder','dream','stars','sky','sea','mystery','unknown','infinite','astral','moon'],\n",
        "    'longing': ['long','yearn','miss','distance','far','wait','absence','hunger','crave'],\n",
        "}\n",
        "word_pat = re.compile(r\"[a-zA-Z']+\")\n",
        "\n",
        "def emotion_vector(text: str):\n",
        "    tokens = [t.lower() for t in word_pat.findall(text)]\n",
        "    if not tokens:\n",
        "        return np.zeros(len(emotion_bins))\n",
        "    counts = Counter(tokens)\n",
        "    vec = np.array([sum(counts[w] for w in words) for words in emotion_bins.values()], dtype=float)\n",
        "    if vec.sum() == 0:\n",
        "        return vec\n",
        "    return vec / (np.linalg.norm(vec) + 1e-8)\n",
        "\n",
        "poem_emotions = np.stack([emotion_vector(t) for t in poem_texts])\n",
        "song_emotions = np.stack([emotion_vector(song_lyrics[int(raw_idx)]) for raw_idx in songs_source_indexes])\n",
        "\n",
        "def shortlist_by_cosine(p_idx, top_k=40):\n",
        "    scores = song_norm @ poem_norm[p_idx]\n",
        "    idxs = scores.argsort()[-top_k:][::-1]\n",
        "    return idxs, scores[idxs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b022e19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to load Qwen model from Qwen/Qwen2.5-7B-Instruct\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60dab153cfe14b18b5cc336d5c8a517a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be9893ed22bd4ba39ae6959079064cf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5571dd4cb61431c98a1bf6dec6e091a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ff94baeb4ea457e9fcb44df36bd267a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e893bc59026441d95764a8d6ca683b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26aee55832734b609c8bb256e1cd88a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2781ee1db7d34d6dae25992a6691f27e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77cba7b8aace455d80d936f9f33c300c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Qwen model (local). Set QWEN_MODEL_PATH or QWEN_MODEL_ID to your local model.\n",
        "model_id = os.getenv('QWEN_MODEL_PATH') or os.getenv('QWEN_MODEL_ID') or 'Qwen/Qwen2.5-7B-Instruct'\n",
        "print('Attempting to load Qwen model from', model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Try to use accelerate/device_map if available; otherwise load and move model to device.\n",
        "try:\n",
        "    # If accelerate is installed, device_map='auto' will work and place weights appropriately.\n",
        "    import accelerate  # noqa: F401\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype, device_map='auto')\n",
        "except Exception:\n",
        "    # Fallback: load on CPU (or default) then move to chosen device\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
        "    model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def qwen_score(poem_text: str, song_text: str, poem_title: str = '', song_title: str = '', song_artist: str = '') -> float:\n",
        "    prompt = (\n",
        "        \"You are evaluating how well a song matches a poem in tone, mood, and theme.\\n\"\n",
        "        f\"Poem title: {poem_title}\\nPoem: {poem_text}\\n\"\n",
        "        f\"Song title: {song_title}\\nArtist: {song_artist}\\nLyrics: {song_text}\\n\"\n",
        "        \"Respond with a single number between 0 and 1 indicating match quality (higher is better).\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=4)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract last number in the response\n",
        "    nums = re.findall(r\"0\\.\\d+|1\\.0+|1\\b\", decoded)\n",
        "    if nums:\n",
        "        return float(nums[-1])\n",
        "    return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80710172",
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "num_poems = 50\n",
        "poem_indices = list(np.linspace(0, len(poems)-1, num_poems, dtype=int))\n",
        "\n",
        "results = []\n",
        "for p_idx in poem_indices:\n",
        "    print(f'Poem {p_idx} / {len(poems)}: {poem_titles[p_idx][:50]}...')\n",
        "    shortlist, _ = shortlist_by_cosine(p_idx, top_k=40)\n",
        "    scored = []\n",
        "    for j, s_idx in enumerate(shortlist):\n",
        "        if j % 10 == 0:\n",
        "            print(f'  scoring song {j+1}/{len(shortlist)}')\n",
        "        raw_idx = int(songs_source_indexes[s_idx])\n",
        "        score = qwen_score(\n",
        "            poem_text=poem_texts[p_idx],\n",
        "            song_text=song_lyrics[raw_idx],\n",
        "            poem_title=poem_titles[p_idx],\n",
        "            song_title=song_titles[raw_idx],\n",
        "            song_artist=song_artists[raw_idx],\n",
        "        )\n",
        "        scored.append((score, s_idx, raw_idx))\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "    top5 = []\n",
        "    seen = set()\n",
        "    for sc, s_idx, raw_idx in scored:\n",
        "        if s_idx in seen:\n",
        "            continue\n",
        "        seen.add(int(s_idx))\n",
        "        top5.append((sc, s_idx, raw_idx))\n",
        "        if len(top5) >= 5:\n",
        "            break\n",
        "    for sc, s_idx, raw_idx in top5:\n",
        "        results.append({\n",
        "            'poem_index': int(p_idx),\n",
        "            'song_index_aligned': int(s_idx),\n",
        "            'label': 1,\n",
        "            'poem_title': poem_titles[p_idx],\n",
        "            'song_title': song_titles[raw_idx],\n",
        "            'song_artist': song_artists[raw_idx],\n",
        "            'qwen_score': float(sc),\n",
        "        })\n",
        "print(f'Collected {len(results)} pairs')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recursive pruning: drop lowest-scoring pairs globally and refill from remaining shortlist\n",
        "pairs_by_poem = {}\n",
        "for r in results:\n",
        "    pairs_by_poem.setdefault(r['poem_index'], []).append(r)\n",
        "\n",
        "def refill(poem_idx, keep_n=5):\n",
        "    current = pairs_by_poem.get(poem_idx, [])\n",
        "    current.sort(key=lambda x: x['qwen_score'], reverse=True)\n",
        "    pairs_by_poem[poem_idx] = current[:keep_n]\n",
        "\n",
        "for p_idx in list(pairs_by_poem.keys()):\n",
        "    refill(p_idx, keep_n=5)\n",
        "\n",
        "# Save JSONL\n",
        "out_path = DATA_DIR / 'processed' / 'qwen_labels.jsonl'\n",
        "with out_path.open('w', encoding='utf-8') as f:\n",
        "    for p_idx in sorted(pairs_by_poem.keys()):\n",
        "        for entry in pairs_by_poem[p_idx]:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "print(f\"Saved {sum(len(v) for v in pairs_by_poem.values())} pairs to {out_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
