{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xinyuezhang-shirley/cs229FinalProject/blob/main/CS229_ProjectionLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvcom4jd77aj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcT0UQyQn9qU",
    "outputId": "04956265-cf14-42f1-bd85-05563014d01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poems: 3413 items\n",
      "Songs: 2995 items\n",
      "poem_vecs: (3413, 768), song_vecs: (2995, 768)\n",
      "poem_feats: (3413, 6), song_feats: (2995, 6)\n",
      "poem_sem (emo/theme/other): (3413, 9), (3413, 10), (3413, 17)\n",
      "song_sem (emo/theme/other): (2995, 9), (2995, 10), (2995, 17)\n"
     ]
    }
   ],
   "source": [
    "# MPNet embeddings (raw, not yet filtered)\n",
    "poem_vecs = np.load(\"data/processed/mpnet_embeddings_poems.npy\")\n",
    "song_vecs = np.load(\"data/processed/mpnet_embeddings_songs.npy\")\n",
    "\n",
    "# Load all features from full_features.npz\n",
    "full = np.load(\"data/processed/full_features.npz\", allow_pickle=True)\n",
    "\n",
    "# Structural + lexical features (concatenated)\n",
    "poem_struct = full[\"poem_struct\"]  # (3413, 3)\n",
    "poem_lexical = full[\"poem_lexical\"]  # (3413, 3)\n",
    "poem_feats = np.concatenate([poem_struct, poem_lexical], axis=1)  # (3413, 6)\n",
    "\n",
    "song_struct = full[\"song_struct\"]  # (2995, 4)\n",
    "song_lexical = full[\"song_lexical\"]  # (2995, 3)\n",
    "# For songs, only use first 3 structural features to match poems (exclude WPM)\n",
    "song_feats = np.concatenate([song_struct[:, :3], song_lexical], axis=1)  # (2995, 6)\n",
    "\n",
    "# Semantic features\n",
    "poem_sem_all = full[\"poem_semantic\"]  # (3413, 36)\n",
    "song_sem_all = full[\"song_semantic\"]  # (2995, 36)\n",
    "\n",
    "# Split semantic features by groups\n",
    "# emotions(9): 0-9, themes(10): 9-19, other(17): 19-36\n",
    "poem_sem_emo   = poem_sem_all[:, 0:9]\n",
    "poem_sem_theme = poem_sem_all[:, 9:19]\n",
    "poem_sem_other = poem_sem_all[:, 19:36]\n",
    "song_sem_emo   = song_sem_all[:, 0:9]\n",
    "song_sem_theme = song_sem_all[:, 9:19]\n",
    "song_sem_other = song_sem_all[:, 19:36]\n",
    "\n",
    "# Align song embeddings to match cleaned features\n",
    "idx_map = full[\"song_source_indexes\"]  # (2995,) maps cleaned songs -> raw embedding indices\n",
    "song_vecs = song_vecs[idx_map]  # reorder raw embeddings to match cleaned data\n",
    "\n",
    "print(f\"Poems: {poem_vecs.shape[0]} items\")\n",
    "print(f\"Songs: {song_vecs.shape[0]} items\")\n",
    "print(f\"poem_vecs: {poem_vecs.shape}, song_vecs: {song_vecs.shape}\")\n",
    "print(f\"poem_feats: {poem_feats.shape}, song_feats: {song_feats.shape}\")\n",
    "print(f\"poem_sem (emo/theme/other): {poem_sem_emo.shape}, {poem_sem_theme.shape}, {poem_sem_other.shape}\")\n",
    "print(f\"song_sem (emo/theme/other): {song_sem_emo.shape}, {song_sem_theme.shape}, {song_sem_other.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4sNEKioCoDl4"
   },
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pos_pairs, neg_pairs, hard_pairs, size):\n",
    "        \"\"\"\n",
    "        Returns poem/song indices for each sample.\n",
    "        size = number of samples per epoch\n",
    "        \"\"\"\n",
    "        self.pos_pairs  = pos_pairs\n",
    "        self.neg_pairs  = neg_pairs\n",
    "        self.hard_pairs = hard_pairs\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Sample random positive pair\n",
    "        i_poem, j_song = self.pos_pairs[np.random.randint(len(self.pos_pairs))]\n",
    "        \n",
    "        # Return indices only (training loop will index the actual data)\n",
    "        return i_poem, j_song\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x7U7AM1ToG3D"
   },
   "outputs": [],
   "source": [
    "# Branch sizes\n",
    "p_dim_mp = poem_in[\"mpnet\"].shape[1]\n",
    "p_dim_emo = poem_in[\"sem_emo\"].shape[1]\n",
    "p_dim_theme = poem_in[\"sem_theme\"].shape[1]\n",
    "p_dim_other = poem_in[\"sem_other\"].shape[1]\n",
    "p_dim_ft  = poem_in[\"feat\"].shape[1]\n",
    "s_dim_mp = song_in[\"mpnet\"].shape[1]\n",
    "s_dim_emo = song_in[\"sem_emo\"].shape[1]\n",
    "s_dim_theme = song_in[\"sem_theme\"].shape[1]\n",
    "s_dim_other = song_in[\"sem_other\"].shape[1]\n",
    "s_dim_ft  = song_in[\"feat\"].shape[1]\n",
    "proj_dim = 128\n",
    "\n",
    "class ProjectionModel(nn.Module):\n",
    "    def __init__(self, p_dims, s_dims, proj_dim):\n",
    "        super().__init__()\n",
    "        p_mp, p_emo, p_theme, p_other, p_ft = p_dims\n",
    "        s_mp, s_emo, s_theme, s_other, s_ft = s_dims\n",
    "        # poem branches\n",
    "        self.poem_mp = nn.Sequential(nn.Linear(p_mp, 256), nn.ReLU(), nn.Linear(256, 128))\n",
    "        self.poem_emo = nn.Sequential(nn.Linear(max(p_emo,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_theme = nn.Sequential(nn.Linear(max(p_theme,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_other = nn.Sequential(nn.Linear(max(p_other,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_ft = nn.Sequential(nn.Linear(p_ft, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_proj = nn.Sequential(nn.LayerNorm(128+64+64+64+64), nn.Linear(128+64+64+64+64, proj_dim))\n",
    "        # song branches\n",
    "        self.song_mp = nn.Sequential(nn.Linear(s_mp, 256), nn.ReLU(), nn.Linear(256, 128))\n",
    "        self.song_emo = nn.Sequential(nn.Linear(max(s_emo,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_theme = nn.Sequential(nn.Linear(max(s_theme,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_other = nn.Sequential(nn.Linear(max(s_other,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_ft = nn.Sequential(nn.Linear(s_ft, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_proj = nn.Sequential(nn.LayerNorm(128+64+64+64+64), nn.Linear(128+64+64+64+64, proj_dim))\n",
    "    def forward_poem(self, p):\n",
    "        mp = self.poem_mp(p[\"mpnet\"])\n",
    "        emo_in = p[\"sem_emo\"] if p_dim_emo>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        theme_in = p[\"sem_theme\"] if p_dim_theme>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        other_in = p[\"sem_other\"] if p_dim_other>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        emo = self.poem_emo(emo_in)\n",
    "        theme = self.poem_theme(theme_in)\n",
    "        other = self.poem_other(other_in)\n",
    "        ft  = self.poem_ft(p[\"feat\"])\n",
    "        comb = torch.cat([ALPHA*mp, BETA_EMO*emo, BETA_THEME*theme, BETA_OTHER*other, GAMMA*ft], dim=1)\n",
    "        z = self.poem_proj(comb)\n",
    "        return F.normalize(z, dim=1)\n",
    "    def forward_song(self, s):\n",
    "        mp = self.song_mp(s[\"mpnet\"])\n",
    "        emo_in = s[\"sem_emo\"] if s_dim_emo>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        theme_in = s[\"sem_theme\"] if s_dim_theme>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        other_in = s[\"sem_other\"] if s_dim_other>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        emo = self.song_emo(emo_in)\n",
    "        theme = self.song_theme(theme_in)\n",
    "        other = self.song_other(other_in)\n",
    "        ft  = self.song_ft(s[\"feat\"])\n",
    "        comb = torch.cat([ALPHA*mp, BETA_EMO*emo, BETA_THEME*theme, BETA_OTHER*other, GAMMA*ft], dim=1)\n",
    "        z = self.song_proj(comb)\n",
    "        return F.normalize(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eChWn15JoIUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from model_bs256_ep500_lr0.002_temp0.1_posK10_hardK20_a0.6_bemo0.1_bthm0.15_both0.1_g0.15.pt...\n",
      "Model loaded. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Build model filename from hyperparameters\n",
    "model_name = (f\"model_bs{BATCH_SIZE}_ep{EPOCHS}_lr{LR}_temp{TEMP}_\"\n",
    "             f\"posK{POS_TOPK}_hardK{HARD_TOPK}_\"\n",
    "             f\"a{ALPHA}_bemo{BETA_EMO}_bthm{BETA_THEME}_both{BETA_OTHER}_g{GAMMA}.pt\")\n",
    "\n",
    "model = ProjectionModel(\n",
    "    (p_dim_mp, p_dim_emo, p_dim_theme, p_dim_other, p_dim_ft),\n",
    "    (s_dim_mp, s_dim_emo, s_dim_theme, s_dim_other, s_dim_ft),\n",
    "    proj_dim\n",
    ").to(DEVICE)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(model_name):\n",
    "    print(f\"Loading existing model from {model_name}...\")\n",
    "    model.load_state_dict(torch.load(model_name, map_location=DEVICE))\n",
    "    print(\"Model loaded. Skipping training.\")\n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    print(f\"No existing model found. Will train and save to {model_name}\")\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Set up schedulers (created even if may not be used to keep code simple)\n",
    "if USE_COSINE_SCHEDULE and not SKIP_TRAINING:\n",
    "    # Warmup: scale LR from (1/WARMUP_EPOCHS)*LR to LR\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / WARMUP_EPOCHS\n",
    "        return 1.0\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "    # Cosine anneal after warmup\n",
    "    scheduler_main = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=MIN_LR)\n",
    "else:\n",
    "    warmup_scheduler = None\n",
    "    scheduler_main = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX3hOUhjoMKl",
    "outputId": "1420d5bd-f37b-4126-9336-836833744694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already loaded; no new save needed.\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f\"Model saved to {model_name}\")\n",
    "else:\n",
    "    print(\"Model already loaded; no new save needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 46 valid human-labeled triplets for supervised training\n",
      "Example: poem=231, song1=550, song2=2335, label=1\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Human-Labeled Test Set\n",
    "Evaluate the unsupervised model on human-labeled triplets to measure real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate unsupervised model on all human triplets\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = len(human_triplets)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p_idx, s1_idx, s2_idx, label in human_triplets:\n",
    "        # Get embeddings for poem and both songs\n",
    "        p_batch = {\n",
    "            \"mpnet\": poem_gpu[\"mpnet\"][p_idx:p_idx+1],\n",
    "            \"sem_emo\": poem_gpu[\"sem_emo\"][p_idx:p_idx+1],\n",
    "            \"sem_theme\": poem_gpu[\"sem_theme\"][p_idx:p_idx+1],\n",
    "            \"sem_other\": poem_gpu[\"sem_other\"][p_idx:p_idx+1],\n",
    "            \"feat\": poem_gpu[\"feat\"][p_idx:p_idx+1],\n",
    "        }\n",
    "        s1_batch = {\n",
    "            \"mpnet\": song_gpu[\"mpnet\"][s1_idx:s1_idx+1],\n",
    "            \"sem_emo\": song_gpu[\"sem_emo\"][s1_idx:s1_idx+1],\n",
    "            \"sem_theme\": song_gpu[\"sem_theme\"][s1_idx:s1_idx+1],\n",
    "            \"sem_other\": song_gpu[\"sem_other\"][s1_idx:s1_idx+1],\n",
    "            \"feat\": song_gpu[\"feat\"][s1_idx:s1_idx+1],\n",
    "        }\n",
    "        s2_batch = {\n",
    "            \"mpnet\": song_gpu[\"mpnet\"][s2_idx:s2_idx+1],\n",
    "            \"sem_emo\": song_gpu[\"sem_emo\"][s2_idx:s2_idx+1],\n",
    "            \"sem_theme\": song_gpu[\"sem_theme\"][s2_idx:s2_idx+1],\n",
    "            \"sem_other\": song_gpu[\"sem_other\"][s2_idx:s2_idx+1],\n",
    "            \"feat\": song_gpu[\"feat\"][s2_idx:s2_idx+1],\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        p_z = model.forward_poem(p_batch)\n",
    "        s1_z = model.forward_song(s1_batch)\n",
    "        s2_z = model.forward_song(s2_batch)\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        sim1 = (p_z * s1_z).sum().item()\n",
    "        sim2 = (p_z * s2_z).sum().item()\n",
    "        \n",
    "        # Predict which song is closer (1 or 2)\n",
    "        pred = 1 if sim1 > sim2 else 2\n",
    "        \n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Unsupervised Model Evaluation on Human Triplets\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total triplets: {total}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Test raw MPNet performance (no training)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE: Raw MPNet Performance (Before Training)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mpnet_correct = 0\n",
    "with torch.no_grad():\n",
    "    for p_idx, s1_idx, s2_idx, label in human_triplets:\n",
    "        # Raw MPNet cosine similarity (normalized embeddings)\n",
    "        p_mpnet = poem_gpu[\"mpnet\"][p_idx:p_idx+1]\n",
    "        s1_mpnet = song_gpu[\"mpnet\"][s1_idx:s1_idx+1]\n",
    "        s2_mpnet = song_gpu[\"mpnet\"][s2_idx:s2_idx+1]\n",
    "        \n",
    "        sim1 = (p_mpnet * s1_mpnet).sum().item()\n",
    "        sim2 = (p_mpnet * s2_mpnet).sum().item()\n",
    "        \n",
    "        pred = 1 if sim1 > sim2 else 2\n",
    "        if pred == label:\n",
    "            mpnet_correct += 1\n",
    "\n",
    "mpnet_acc = mpnet_correct / len(human_triplets)\n",
    "print(f\"Raw MPNet Accuracy: {mpnet_acc*100:.2f}% ({mpnet_correct}/{len(human_triplets)})\")\n",
    "print(f\"Your Model Accuracy: 43.48% (20/{len(human_triplets)})\")\n",
    "print(f\"\\nPerformance Loss: {(mpnet_acc - 0.4348)*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label encoding and predictions for first few examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG: First 5 triplet predictions (check for off-by-one)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (p_idx, s1_idx, s2_idx, label) in enumerate(human_triplets[:5]):\n",
    "        # Raw MPNet\n",
    "        p_mpnet = poem_gpu[\"mpnet\"][p_idx:p_idx+1]\n",
    "        s1_mpnet = song_gpu[\"mpnet\"][s1_idx:s1_idx+1]\n",
    "        s2_mpnet = song_gpu[\"mpnet\"][s2_idx:s2_idx+1]\n",
    "        \n",
    "        mpnet_sim1 = (p_mpnet * s1_mpnet).sum().item()\n",
    "        mpnet_sim2 = (p_mpnet * s2_mpnet).sum().item()\n",
    "        mpnet_pred = 1 if mpnet_sim1 > mpnet_sim2 else 2\n",
    "        \n",
    "        # Model\n",
    "        p_batch = {\"mpnet\": poem_gpu[\"mpnet\"][p_idx:p_idx+1],\n",
    "                   \"sem_emo\": poem_gpu[\"sem_emo\"][p_idx:p_idx+1],\n",
    "                   \"sem_theme\": poem_gpu[\"sem_theme\"][p_idx:p_idx+1],\n",
    "                   \"sem_other\": poem_gpu[\"sem_other\"][p_idx:p_idx+1],\n",
    "                   \"feat\": poem_gpu[\"feat\"][p_idx:p_idx+1]}\n",
    "        s1_batch = {\"mpnet\": song_gpu[\"mpnet\"][s1_idx:s1_idx+1],\n",
    "                    \"sem_emo\": song_gpu[\"sem_emo\"][s1_idx:s1_idx+1],\n",
    "                    \"sem_theme\": song_gpu[\"sem_theme\"][s1_idx:s1_idx+1],\n",
    "                    \"sem_other\": song_gpu[\"sem_other\"][s1_idx:s1_idx+1],\n",
    "                    \"feat\": song_gpu[\"feat\"][s1_idx:s1_idx+1]}\n",
    "        s2_batch = {\"mpnet\": song_gpu[\"mpnet\"][s2_idx:s2_idx+1],\n",
    "                    \"sem_emo\": song_gpu[\"sem_emo\"][s2_idx:s2_idx+1],\n",
    "                    \"sem_theme\": song_gpu[\"sem_theme\"][s2_idx:s2_idx+1],\n",
    "                    \"sem_other\": song_gpu[\"sem_other\"][s2_idx:s2_idx+1],\n",
    "                    \"feat\": song_gpu[\"feat\"][s2_idx:s2_idx+1]}\n",
    "        \n",
    "        p_z = model.forward_poem(p_batch)\n",
    "        s1_z = model.forward_song(s1_batch)\n",
    "        s2_z = model.forward_song(s2_batch)\n",
    "        \n",
    "        model_sim1 = (p_z * s1_z).sum().item()\n",
    "        model_sim2 = (p_z * s2_z).sum().item()\n",
    "        model_pred = 1 if model_sim1 > model_sim2 else 2\n",
    "        \n",
    "        print(f\"\\nTriplet {idx}: poem={p_idx}, song1={s1_idx}, song2={s2_idx}\")\n",
    "        print(f\"  Label says: Song {label} is closer\")\n",
    "        print(f\"  MPNet: sim1={mpnet_sim1:.3f}, sim2={mpnet_sim2:.3f} → pred={mpnet_pred} {'✓' if mpnet_pred==label else '✗'}\")\n",
    "        print(f\"  Model: sim1={model_sim1:.3f}, sim2={model_sim2:.3f} → pred={model_pred} {'✓' if model_pred==label else '✗'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If MPNet is mostly ✓ but Model is mostly ✗, training degraded performance\")\n",
    "print(\"If both are ✗, check if labels are backwards (flip 1↔2)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPhkjxY/YsXdDuc4G7l52t2",
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
