{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xinyuezhang-shirley/cs229FinalProject/blob/main/CS229_ProjectionLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvcom4jd77aj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5h2gjc-n72X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 128                \n",
    "EPOCHS = 500              # maximum epochs (early stop may cut short)\n",
    "LR = 2e-3                 # base learning rate (after warmup)\n",
    "TEMP = 0.7                # InfoNCE temperature\n",
    "SAMPLES_PER_EPOCH = 500   # reduced from 10k to keep training fast with batch_size=2\n",
    "\n",
    "# Learning rate schedule / early stopping\n",
    "WARMUP_EPOCHS = 5         # linear warmup epochs\n",
    "MIN_LR = 1e-3             # final minimum LR for cosine schedule\n",
    "USE_COSINE_SCHEDULE = True\n",
    "EARLY_STOP_PATIENCE = 25  # epochs (post-warmup) with no sufficient improvement\n",
    "EARLY_STOP_DELTA = 0.002  # required loss decrease to reset patience\n",
    "MOVING_AVG_WINDOW = 10    # for smoothed loss\n",
    "\n",
    "# Modality weights (kept same)\n",
    "ALPHA = 0.6       # MPNet branch\n",
    "BETA_EMO = 0.1    # emotion semantics\n",
    "BETA_THEME = 0.15 # theme semantics\n",
    "BETA_OTHER = 0.1  # other semantics (sentiment, subjectivity, concreteness, energy, narrative, imagery)\n",
    "GAMMA = 0.15      # structural/lexical branch\n",
    "\n",
    "# Unsupervised pair construction hyperparameters\n",
    "POS_TOPK = 5        # positives per poem from similarity\n",
    "HARD_TOPK = 10       # hard negatives per poem (near misses)\n",
    "EASY_THRESHOLD = 0.25  # cosine threshold for easy negatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcT0UQyQn9qU",
    "outputId": "04956265-cf14-42f1-bd85-05563014d01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poems: 3413 items\n",
      "Songs: 2995 items\n",
      "poem_vecs: (3413, 768), song_vecs: (2995, 768)\n",
      "poem_feats: (3413, 6), song_feats: (2995, 6)\n",
      "poem_sem (emo/theme/other): (3413, 9), (3413, 10), (3413, 17)\n",
      "song_sem (emo/theme/other): (2995, 9), (2995, 10), (2995, 17)\n"
     ]
    }
   ],
   "source": [
    "# MPNet embeddings (raw, not yet filtered)\n",
    "poem_vecs = np.load(\"data/processed/mpnet_embeddings_poems.npy\")\n",
    "song_vecs = np.load(\"data/processed/mpnet_embeddings_songs.npy\")\n",
    "\n",
    "# Load all features from full_features.npz\n",
    "full = np.load(\"data/processed/full_features.npz\", allow_pickle=True)\n",
    "\n",
    "# Structural + lexical features (concatenated)\n",
    "poem_struct = full[\"poem_struct\"]  # (3413, 3)\n",
    "poem_lexical = full[\"poem_lexical\"]  # (3413, 3)\n",
    "poem_feats = np.concatenate([poem_struct, poem_lexical], axis=1)  # (3413, 6)\n",
    "\n",
    "song_struct = full[\"song_struct\"]  # (2995, 4)\n",
    "song_lexical = full[\"song_lexical\"]  # (2995, 3)\n",
    "# For songs, only use first 3 structural features to match poems (exclude WPM)\n",
    "song_feats = np.concatenate([song_struct[:, :3], song_lexical], axis=1)  # (2995, 6)\n",
    "\n",
    "# Semantic features\n",
    "poem_sem_all = full[\"poem_semantic\"]  # (3413, 36)\n",
    "song_sem_all = full[\"song_semantic\"]  # (2995, 36)\n",
    "\n",
    "# Split semantic features by groups\n",
    "# emotions(9): 0-9, themes(10): 9-19, other(17): 19-36\n",
    "poem_sem_emo   = poem_sem_all[:, 0:9]\n",
    "poem_sem_theme = poem_sem_all[:, 9:19]\n",
    "poem_sem_other = poem_sem_all[:, 19:36]\n",
    "song_sem_emo   = song_sem_all[:, 0:9]\n",
    "song_sem_theme = song_sem_all[:, 9:19]\n",
    "song_sem_other = song_sem_all[:, 19:36]\n",
    "\n",
    "# Align song embeddings to match cleaned features\n",
    "idx_map = full[\"song_source_indexes\"]  # (2995,) maps cleaned songs -> raw embedding indices\n",
    "song_vecs = song_vecs[idx_map]  # reorder raw embeddings to match cleaned data\n",
    "\n",
    "print(f\"Poems: {poem_vecs.shape[0]} items\")\n",
    "print(f\"Songs: {song_vecs.shape[0]} items\")\n",
    "print(f\"poem_vecs: {poem_vecs.shape}, song_vecs: {song_vecs.shape}\")\n",
    "print(f\"poem_feats: {poem_feats.shape}, song_feats: {song_feats.shape}\")\n",
    "print(f\"poem_sem (emo/theme/other): {poem_sem_emo.shape}, {poem_sem_theme.shape}, {poem_sem_other.shape}\")\n",
    "print(f\"song_sem (emo/theme/other): {song_sem_emo.shape}, {song_sem_theme.shape}, {song_sem_other.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kn-OMdToB8A",
    "outputId": "12710210-85d8-45d7-a3ff-d73f088e125a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poem branches: (3413, 768) (3413, 9) (3413, 10) (3413, 17) (3413, 6)\n",
      "song  branches: (2995, 768) (2995, 9) (2995, 10) (2995, 17) (2995, 6)\n",
      "Loading precomputed cosine matrix from data/processed/mpnet_pairwise_cosine_matrix.npy...\n",
      "Loaded matrix shape: torch.Size([3413, 2995])\n",
      "Building pairs with POS_TOPK=10, HARD_TOPK=20, EASY_THRESHOLD=0.15...\n",
      "Built pairs -> pos: 34130 hard: 68260 easy: 16207\n"
     ]
    }
   ],
   "source": [
    "# Normalize MPNet embeddings per row to balance scales\n",
    "poem_vecs = poem_vecs / (np.linalg.norm(poem_vecs, axis=1, keepdims=True) + 1e-8)\n",
    "song_vecs = song_vecs / (np.linalg.norm(song_vecs, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# Build branch inputs\n",
    "poem_in = {\n",
    "    \"mpnet\": poem_vecs,\n",
    "    \"sem_emo\":   poem_sem_emo,\n",
    "    \"sem_theme\": poem_sem_theme,\n",
    "    \"sem_other\": poem_sem_other,\n",
    "    \"feat\":  poem_feats,\n",
    "}\n",
    "song_in = {\n",
    "    \"mpnet\": song_vecs,\n",
    "    \"sem_emo\":   song_sem_emo,\n",
    "    \"sem_theme\": song_sem_theme,\n",
    "    \"sem_other\": song_sem_other,\n",
    "    \"feat\":  song_feats,\n",
    "}\n",
    "\n",
    "print(\"poem branches:\", poem_in[\"mpnet\"].shape, poem_in[\"sem_emo\"].shape, poem_in[\"sem_theme\"].shape, poem_in[\"sem_other\"].shape, poem_in[\"feat\"].shape)\n",
    "print(\"song  branches:\", song_in[\"mpnet\"].shape, song_in[\"sem_emo\"].shape, song_in[\"sem_theme\"].shape, song_in[\"sem_other\"].shape, song_in[\"feat\"].shape)\n",
    "\n",
    "# Compute or load precomputed pairwise cosine similarity matrix\n",
    "import torch\n",
    "import os\n",
    "\n",
    "cosine_matrix_path = \"data/processed/mpnet_pairwise_cosine_matrix.npy\"\n",
    "\n",
    "if os.path.exists(cosine_matrix_path):\n",
    "    print(f\"Loading precomputed cosine matrix from {cosine_matrix_path}...\")\n",
    "    cos_matrix_t = torch.from_numpy(np.load(cosine_matrix_path)).to(DEVICE)\n",
    "    print(f\"Loaded matrix shape: {cos_matrix_t.shape}\")\n",
    "else:\n",
    "    print(\"Computing pairwise cosine similarity matrix on GPU...\")\n",
    "    # Move MPNet branches to GPU tensors\n",
    "    p_mp = torch.from_numpy(poem_in[\"mpnet\"]).to(torch.float32).to(DEVICE)\n",
    "    s_mp = torch.from_numpy(song_in[\"mpnet\"]).to(torch.float32).to(DEVICE)\n",
    "    \n",
    "    # Compute cosine similarity matrix via matmul on GPU (all poem-song pairs)\n",
    "    cos_matrix_t = torch.matmul(p_mp, s_mp.T)  # [P, S]\n",
    "    \n",
    "    # Save to cache\n",
    "    np.save(cosine_matrix_path, cos_matrix_t.cpu().numpy())\n",
    "    print(f\"Saved cosine matrix to {cosine_matrix_path}\")\n",
    "\n",
    "# Build pos/hard/neg pairs from current hyperparameters (always recompute based on thresholds)\n",
    "print(f\"Building pairs with POS_TOPK={POS_TOPK}, HARD_TOPK={HARD_TOPK}, EASY_THRESHOLD={EASY_THRESHOLD}...\")\n",
    "P, S = cos_matrix_t.shape\n",
    "pos_pairs = []\n",
    "hard_pairs = []\n",
    "neg_pairs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # For each poem, get top (POS_TOPK + HARD_TOPK) indices\n",
    "    topk_vals, topk_idxs = torch.topk(cos_matrix_t, k=min(S, POS_TOPK + HARD_TOPK), dim=1, largest=True, sorted=True)\n",
    "\n",
    "    # Build pos and hard lists\n",
    "    for i in range(P):\n",
    "        # positives\n",
    "        for j in topk_idxs[i, :POS_TOPK].tolist():\n",
    "            pos_pairs.append((int(i), int(j)))\n",
    "        # hard negatives (near misses)\n",
    "        for j in topk_idxs[i, POS_TOPK:POS_TOPK+HARD_TOPK].tolist():\n",
    "            hard_pairs.append((int(i), int(j)))\n",
    "\n",
    "    # Easy negatives: cosine below threshold; sample a few per poem\n",
    "    easy_mask = cos_matrix_t <= EASY_THRESHOLD\n",
    "    for i in range(P):\n",
    "        low_idxs = torch.nonzero(easy_mask[i], as_tuple=False).squeeze(-1).cpu().numpy()\n",
    "        if low_idxs.size > 0:\n",
    "            sample_ct = min(5, low_idxs.size)\n",
    "            choice = np.random.choice(low_idxs, size=sample_ct, replace=False)\n",
    "            for j in choice:\n",
    "                neg_pairs.append((int(i), int(j)))\n",
    "\n",
    "print(f\"Built pairs -> pos: {len(pos_pairs)} hard: {len(hard_pairs)} easy: {len(neg_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4sNEKioCoDl4"
   },
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pos_pairs, neg_pairs, hard_pairs, size):\n",
    "        \"\"\"\n",
    "        Returns poem/song indices for each sample.\n",
    "        size = number of samples per epoch\n",
    "        \"\"\"\n",
    "        self.pos_pairs  = pos_pairs\n",
    "        self.neg_pairs  = neg_pairs\n",
    "        self.hard_pairs = hard_pairs\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Sample random positive pair\n",
    "        i_poem, j_song = self.pos_pairs[np.random.randint(len(self.pos_pairs))]\n",
    "        \n",
    "        # Return indices only (training loop will index the actual data)\n",
    "        return i_poem, j_song\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3YpZnvxUoFBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-built GPU tensors for 3413 poems and 2995 songs\n"
     ]
    }
   ],
   "source": [
    "dataset = PairDataset(\n",
    "    pos_pairs,\n",
    "    neg_pairs,\n",
    "    hard_pairs,\n",
    "    size=SAMPLES_PER_EPOCH\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for simpler debugging; increase on CPU\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Pre-build GPU tensors for all data (avoids repeated CPU->GPU transfers)\n",
    "poem_gpu = {\n",
    "    \"mpnet\": torch.from_numpy(poem_in[\"mpnet\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_emo\": torch.from_numpy(poem_in[\"sem_emo\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_theme\": torch.from_numpy(poem_in[\"sem_theme\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_other\": torch.from_numpy(poem_in[\"sem_other\"]).to(torch.float32).to(DEVICE),\n",
    "    \"feat\": torch.from_numpy(poem_in[\"feat\"]).to(torch.float32).to(DEVICE),\n",
    "}\n",
    "song_gpu = {\n",
    "    \"mpnet\": torch.from_numpy(song_in[\"mpnet\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_emo\": torch.from_numpy(song_in[\"sem_emo\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_theme\": torch.from_numpy(song_in[\"sem_theme\"]).to(torch.float32).to(DEVICE),\n",
    "    \"sem_other\": torch.from_numpy(song_in[\"sem_other\"]).to(torch.float32).to(DEVICE),\n",
    "    \"feat\": torch.from_numpy(song_in[\"feat\"]).to(torch.float32).to(DEVICE),\n",
    "}\n",
    "\n",
    "print(f\"Pre-built GPU tensors for {poem_gpu['mpnet'].shape[0]} poems and {song_gpu['mpnet'].shape[0]} songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x7U7AM1ToG3D"
   },
   "outputs": [],
   "source": [
    "# Branch sizes\n",
    "p_dim_mp = poem_in[\"mpnet\"].shape[1]\n",
    "p_dim_emo = poem_in[\"sem_emo\"].shape[1]\n",
    "p_dim_theme = poem_in[\"sem_theme\"].shape[1]\n",
    "p_dim_other = poem_in[\"sem_other\"].shape[1]\n",
    "p_dim_ft  = poem_in[\"feat\"].shape[1]\n",
    "s_dim_mp = song_in[\"mpnet\"].shape[1]\n",
    "s_dim_emo = song_in[\"sem_emo\"].shape[1]\n",
    "s_dim_theme = song_in[\"sem_theme\"].shape[1]\n",
    "s_dim_other = song_in[\"sem_other\"].shape[1]\n",
    "s_dim_ft  = song_in[\"feat\"].shape[1]\n",
    "proj_dim = 128\n",
    "\n",
    "class ProjectionModel(nn.Module):\n",
    "    def __init__(self, p_dims, s_dims, proj_dim):\n",
    "        super().__init__()\n",
    "        p_mp, p_emo, p_theme, p_other, p_ft = p_dims\n",
    "        s_mp, s_emo, s_theme, s_other, s_ft = s_dims\n",
    "        # poem branches\n",
    "        self.poem_mp = nn.Sequential(nn.Linear(p_mp, 256), nn.ReLU(), nn.Linear(256, 128))\n",
    "        self.poem_emo = nn.Sequential(nn.Linear(max(p_emo,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_theme = nn.Sequential(nn.Linear(max(p_theme,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_other = nn.Sequential(nn.Linear(max(p_other,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_ft = nn.Sequential(nn.Linear(p_ft, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.poem_proj = nn.Sequential(nn.LayerNorm(128+64+64+64+64), nn.Linear(128+64+64+64+64, proj_dim))\n",
    "        # song branches\n",
    "        self.song_mp = nn.Sequential(nn.Linear(s_mp, 256), nn.ReLU(), nn.Linear(256, 128))\n",
    "        self.song_emo = nn.Sequential(nn.Linear(max(s_emo,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_theme = nn.Sequential(nn.Linear(max(s_theme,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_other = nn.Sequential(nn.Linear(max(s_other,1), 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_ft = nn.Sequential(nn.Linear(s_ft, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        self.song_proj = nn.Sequential(nn.LayerNorm(128+64+64+64+64), nn.Linear(128+64+64+64+64, proj_dim))\n",
    "    def forward_poem(self, p):\n",
    "        mp = self.poem_mp(p[\"mpnet\"])\n",
    "        emo_in = p[\"sem_emo\"] if p_dim_emo>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        theme_in = p[\"sem_theme\"] if p_dim_theme>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        other_in = p[\"sem_other\"] if p_dim_other>0 else torch.zeros(p[\"mpnet\"].shape[0], 1, device=p[\"mpnet\"].device)\n",
    "        emo = self.poem_emo(emo_in)\n",
    "        theme = self.poem_theme(theme_in)\n",
    "        other = self.poem_other(other_in)\n",
    "        ft  = self.poem_ft(p[\"feat\"])\n",
    "        comb = torch.cat([ALPHA*mp, BETA_EMO*emo, BETA_THEME*theme, BETA_OTHER*other, GAMMA*ft], dim=1)\n",
    "        z = self.poem_proj(comb)\n",
    "        return F.normalize(z, dim=1)\n",
    "    def forward_song(self, s):\n",
    "        mp = self.song_mp(s[\"mpnet\"])\n",
    "        emo_in = s[\"sem_emo\"] if s_dim_emo>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        theme_in = s[\"sem_theme\"] if s_dim_theme>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        other_in = s[\"sem_other\"] if s_dim_other>0 else torch.zeros(s[\"mpnet\"].shape[0], 1, device=s[\"mpnet\"].device)\n",
    "        emo = self.song_emo(emo_in)\n",
    "        theme = self.song_theme(theme_in)\n",
    "        other = self.song_other(other_in)\n",
    "        ft  = self.song_ft(s[\"feat\"])\n",
    "        comb = torch.cat([ALPHA*mp, BETA_EMO*emo, BETA_THEME*theme, BETA_OTHER*other, GAMMA*ft], dim=1)\n",
    "        z = self.song_proj(comb)\n",
    "        return F.normalize(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gC9wIi4XtidP"
   },
   "outputs": [],
   "source": [
    "def clip_loss(p_z, s_z, temperature=TEMP):\n",
    "    # InfoNCE symmetric loss\n",
    "    logits = (p_z @ s_z.T) / temperature\n",
    "    labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "    loss_p_to_s = F.cross_entropy(logits, labels)\n",
    "    loss_s_to_p = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_p_to_s + loss_s_to_p) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eChWn15JoIUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from model_bs256_ep500_lr0.002_temp0.1_posK10_hardK20_a0.6_bemo0.1_bthm0.15_both0.1_g0.15.pt...\n",
      "Model loaded. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Build model filename from hyperparameters\n",
    "model_name = (f\"model_bs{BATCH_SIZE}_ep{EPOCHS}_lr{LR}_temp{TEMP}_\"\n",
    "             f\"posK{POS_TOPK}_hardK{HARD_TOPK}_\"\n",
    "             f\"a{ALPHA}_bemo{BETA_EMO}_bthm{BETA_THEME}_both{BETA_OTHER}_g{GAMMA}.pt\")\n",
    "\n",
    "model = ProjectionModel(\n",
    "    (p_dim_mp, p_dim_emo, p_dim_theme, p_dim_other, p_dim_ft),\n",
    "    (s_dim_mp, s_dim_emo, s_dim_theme, s_dim_other, s_dim_ft),\n",
    "    proj_dim\n",
    ").to(DEVICE)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(model_name):\n",
    "    print(f\"Loading existing model from {model_name}...\")\n",
    "    model.load_state_dict(torch.load(model_name, map_location=DEVICE))\n",
    "    print(\"Model loaded. Skipping training.\")\n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    print(f\"No existing model found. Will train and save to {model_name}\")\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Set up schedulers (created even if may not be used to keep code simple)\n",
    "if USE_COSINE_SCHEDULE and not SKIP_TRAINING:\n",
    "    # Warmup: scale LR from (1/WARMUP_EPOCHS)*LR to LR\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / WARMUP_EPOCHS\n",
    "        return 1.0\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "    # Cosine anneal after warmup\n",
    "    scheduler_main = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=MIN_LR)\n",
    "else:\n",
    "    warmup_scheduler = None\n",
    "    scheduler_main = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kqmhbms2oKkL",
    "outputId": "1bc1d3f7-a99a-4808-e461-31c9a15412d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loaded model; skipping training loop.\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    loss_history = []            # raw epoch loss for plotting\n",
    "    lr_history = []\n",
    "    best_smooth = float('inf')   # track best smoothed loss for early stopping\n",
    "    epochs_no_improve = 0\n",
    "    smooth_history = []  # window of recent raw losses for smoothing\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "\n",
    "        for poem_idxs, song_idxs in loader:\n",
    "            # Index pre-built GPU tensors with batch indices\n",
    "            p_batch = {\n",
    "                \"mpnet\": poem_gpu[\"mpnet\"][poem_idxs],\n",
    "                \"sem_emo\": poem_gpu[\"sem_emo\"][poem_idxs],\n",
    "                \"sem_theme\": poem_gpu[\"sem_theme\"][poem_idxs],\n",
    "                \"sem_other\": poem_gpu[\"sem_other\"][poem_idxs],\n",
    "                \"feat\": poem_gpu[\"feat\"][poem_idxs],\n",
    "            }\n",
    "            s_batch = {\n",
    "                \"mpnet\": song_gpu[\"mpnet\"][song_idxs],\n",
    "                \"sem_emo\": song_gpu[\"sem_emo\"][song_idxs],\n",
    "                \"sem_theme\": song_gpu[\"sem_theme\"][song_idxs],\n",
    "                \"sem_other\": song_gpu[\"sem_other\"][song_idxs],\n",
    "                \"feat\": song_gpu[\"feat\"][song_idxs],\n",
    "            }\n",
    "\n",
    "            # Forward\n",
    "            p_z = model.forward_poem(p_batch)\n",
    "            s_z = model.forward_song(s_batch)\n",
    "\n",
    "            # Contrastive loss\n",
    "            loss = clip_loss(p_z, s_z)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "        avg = total / len(loader)          # raw epoch loss\n",
    "        loss_history.append(avg)           # store raw for plotting only\n",
    "\n",
    "        # Update smoothing window and compute smoothed loss\n",
    "        smooth_history.append(avg)\n",
    "        if len(smooth_history) > MOVING_AVG_WINDOW:\n",
    "            smooth_history.pop(0)\n",
    "        smooth_avg = sum(smooth_history) / len(smooth_history)\n",
    "\n",
    "        # Scheduler step\n",
    "        if USE_COSINE_SCHEDULE:\n",
    "            if epoch < WARMUP_EPOCHS:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                scheduler_main.step()\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "        lr_history.append(current_lr)\n",
    "\n",
    "        # Early stopping uses smoothed loss (post-warmup)\n",
    "        if epoch >= WARMUP_EPOCHS:\n",
    "            if smooth_avg < best_smooth - EARLY_STOP_DELTA:\n",
    "                best_smooth = smooth_avg\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}: smoothed loss no improvement > {EARLY_STOP_DELTA} for {EARLY_STOP_PATIENCE} epochs.\")\n",
    "                break\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{EPOCHS} lr={current_lr:.6f} raw_loss={avg:.4f} smooth_loss={smooth_avg:.4f}\")\n",
    "else:\n",
    "    print(\"Using loaded model; skipping training loop.\")\n",
    "    loss_history = []  # empty since no training occurred\n",
    "    lr_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uX3hOUhjoMKl",
    "outputId": "1420d5bd-f37b-4126-9336-836833744694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already loaded; no new save needed.\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAINING:\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f\"Model saved to {model_name}\")\n",
    "else:\n",
    "    print(\"Model already loaded; no new save needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "FlQwL76woN5-",
    "outputId": "8e83f3ba-0266-489c-a568-0063fa5a0fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training occurred (model was loaded); skipping loss plot.\n"
     ]
    }
   ],
   "source": [
    "if len(loss_history) > 0:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(loss_history, linewidth=2)\n",
    "    plt.title(\"Training Loss (Contrastive Only)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training occurred (model was loaded); skipping loss plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 46 valid human-labeled triplets for supervised training\n",
      "Example: poem=231, song1=550, song2=2335, label=1\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Human-Labeled Test Set\n",
    "Evaluate the unsupervised model on human-labeled triplets to measure real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load human-labeled triplets from spreadsheet\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "xlsx_path = \"data/Song_Poem.xlsx\"\n",
    "xls = pd.ExcelFile(xlsx_path)\n",
    "df_human = pd.read_excel(xls, sheet_name=xls.sheet_names[0]).rename(columns=lambda c: str(c).strip())\n",
    "\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "cols_lower_map = {norm(c): c for c in df_human.columns}\n",
    "\n",
    "# Find columns\n",
    "col_label = cols_lower_map.get(\"song number closer to poem\") or cols_lower_map.get(\"song number closer to\")\n",
    "col_poem = cols_lower_map.get(\"poem 1\")\n",
    "col_song1 = cols_lower_map.get(\"song 1\")\n",
    "col_song2 = cols_lower_map.get(\"song 2\")\n",
    "\n",
    "for k, v in cols_lower_map.items():\n",
    "    if col_label is None and k.startswith(\"song number closer\"):\n",
    "        col_label = v\n",
    "    if col_poem is None and k.startswith(\"poem\"):\n",
    "        col_poem = v\n",
    "    if col_song1 is None and k.startswith(\"song 1\"):\n",
    "        col_song1 = v\n",
    "    if col_song2 is None and k.startswith(\"song 2\"):\n",
    "        col_song2 = v\n",
    "\n",
    "int_re = re.compile(r\"(\\d+)\")\n",
    "\n",
    "def parse_id(val):\n",
    "    s = str(val).strip()\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if f.is_integer():\n",
    "            return int(f)\n",
    "    except:\n",
    "        pass\n",
    "    if s.isdigit():\n",
    "        return int(s)\n",
    "    m = int_re.findall(s)\n",
    "    if m:\n",
    "        return int(m[-1])\n",
    "    return None\n",
    "\n",
    "def to_zero_based(idx, n):\n",
    "    if idx is None:\n",
    "        return None\n",
    "    if 0 <= idx < n:\n",
    "        return idx\n",
    "    if 1 <= idx <= n:\n",
    "        return idx - 1\n",
    "    return None\n",
    "\n",
    "poem_count = poem_in[\"mpnet\"].shape[0]\n",
    "song_count = song_in[\"mpnet\"].shape[0]\n",
    "\n",
    "# Extract valid human triplets: (poem_idx, song1_idx, song2_idx, label)\n",
    "human_triplets = []\n",
    "for _, r in df_human.iterrows():\n",
    "    label_str = str(r[col_label]).strip()\n",
    "    try:\n",
    "        label_int = int(float(label_str))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if label_int not in [1, 2]:\n",
    "        continue\n",
    "    \n",
    "    p_id = parse_id(r[col_poem])\n",
    "    s1_id = parse_id(r[col_song1])\n",
    "    s2_id = parse_id(r[col_song2])\n",
    "    p_idx = to_zero_based(p_id, poem_count)\n",
    "    s1_idx = to_zero_based(s1_id, song_count)\n",
    "    s2_idx = to_zero_based(s2_id, song_count)\n",
    "    \n",
    "    if p_idx is not None and s1_idx is not None and s2_idx is not None:\n",
    "        human_triplets.append((p_idx, s1_idx, s2_idx, label_int))\n",
    "\n",
    "print(f\"Loaded {len(human_triplets)} valid human-labeled triplets for evaluation\")\n",
    "print(f\"Example: poem={human_triplets[0][0]}, song1={human_triplets[0][1]}, song2={human_triplets[0][2]}, label={human_triplets[0][3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate unsupervised model on all human triplets\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = len(human_triplets)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p_idx, s1_idx, s2_idx, label in human_triplets:\n",
    "        # Get embeddings for poem and both songs\n",
    "        p_batch = {\n",
    "            \"mpnet\": poem_gpu[\"mpnet\"][p_idx:p_idx+1],\n",
    "            \"sem_emo\": poem_gpu[\"sem_emo\"][p_idx:p_idx+1],\n",
    "            \"sem_theme\": poem_gpu[\"sem_theme\"][p_idx:p_idx+1],\n",
    "            \"sem_other\": poem_gpu[\"sem_other\"][p_idx:p_idx+1],\n",
    "            \"feat\": poem_gpu[\"feat\"][p_idx:p_idx+1],\n",
    "        }\n",
    "        s1_batch = {\n",
    "            \"mpnet\": song_gpu[\"mpnet\"][s1_idx:s1_idx+1],\n",
    "            \"sem_emo\": song_gpu[\"sem_emo\"][s1_idx:s1_idx+1],\n",
    "            \"sem_theme\": song_gpu[\"sem_theme\"][s1_idx:s1_idx+1],\n",
    "            \"sem_other\": song_gpu[\"sem_other\"][s1_idx:s1_idx+1],\n",
    "            \"feat\": song_gpu[\"feat\"][s1_idx:s1_idx+1],\n",
    "        }\n",
    "        s2_batch = {\n",
    "            \"mpnet\": song_gpu[\"mpnet\"][s2_idx:s2_idx+1],\n",
    "            \"sem_emo\": song_gpu[\"sem_emo\"][s2_idx:s2_idx+1],\n",
    "            \"sem_theme\": song_gpu[\"sem_theme\"][s2_idx:s2_idx+1],\n",
    "            \"sem_other\": song_gpu[\"sem_other\"][s2_idx:s2_idx+1],\n",
    "            \"feat\": song_gpu[\"feat\"][s2_idx:s2_idx+1],\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        p_z = model.forward_poem(p_batch)\n",
    "        s1_z = model.forward_song(s1_batch)\n",
    "        s2_z = model.forward_song(s2_batch)\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        sim1 = (p_z * s1_z).sum().item()\n",
    "        sim2 = (p_z * s2_z).sum().item()\n",
    "        \n",
    "        # Predict which song is closer (1 or 2)\n",
    "        pred = 1 if sim1 > sim2 else 2\n",
    "        \n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Unsupervised Model Evaluation on Human Triplets\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total triplets: {total}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPhkjxY/YsXdDuc4G7l52t2",
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
